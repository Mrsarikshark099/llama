{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3087543,"sourceType":"datasetVersion","datasetId":1887384},{"sourceId":9722923,"sourceType":"datasetVersion","datasetId":5949050},{"sourceId":9723111,"sourceType":"datasetVersion","datasetId":5949197},{"sourceId":9723702,"sourceType":"datasetVersion","datasetId":5949622}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import kagglehub\n\n# # Download latest version\n# path = kagglehub.dataset_download(\"awsaf49/food-recognition-2022-dataset\")\n\n# print(\"Path to dataset files:\", path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-25T23:08:42.536266Z","iopub.execute_input":"2024-10-25T23:08:42.537251Z","iopub.status.idle":"2024-10-25T23:08:42.542269Z","shell.execute_reply.started":"2024-10-25T23:08:42.537200Z","shell.execute_reply":"2024-10-25T23:08:42.541347Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"image_data_path = r'/kaggle/input/food-recognition-2022-dataset/public_validation_set_2.0/images/'\nval_annot_path = r'/kaggle/input/wholedataset/xyz/xyz/val.json'","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:08:43.732061Z","iopub.execute_input":"2024-10-25T23:08:43.732855Z","iopub.status.idle":"2024-10-25T23:08:43.741530Z","shell.execute_reply.started":"2024-10-25T23:08:43.732816Z","shell.execute_reply":"2024-10-25T23:08:43.740623Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom collections import OrderedDict\n\nfood = json.load(open(val_annot_path))\ncategories = food[\"categories\"]\nclass_names = [category[\"name\"] for category in categories]\ncategory_ids = [category[\"id\"] for category in categories]\n\nno_images_per_category = {}\nimage_data = [] \n\nfor n, cat_id in enumerate(category_ids):\n    img_ids = [ann['image_id'] for ann in food['annotations'] if ann['category_id'] == cat_id]\n    label = class_names[n]\n    no_images_per_category[label] = len(img_ids)\n\n    for img_id in img_ids:\n        img_info = next((img for img in food['images'] if img['id'] == img_id), None)\n        if img_info:\n            image_data.append({\n                'file_name': img_info['file_name'],\n                'height': img_info['height'],\n                'width': img_info['width'],\n                'id': img_info['id'],\n                'category': label\n            })\n\ndf_image_info = pd.DataFrame(image_data)\n\nno_images_per_category = OrderedDict(sorted(no_images_per_category.items(), key=lambda x: -1 * x[1]))\n\n\ntop_30_categories = []\ni = 0\nfor k, v in no_images_per_category.items():\n    print(k, v)\n    top_30_categories.append((k, v))\n    i += 1\n    if i >= 30:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:08:55.138460Z","iopub.execute_input":"2024-10-25T23:08:55.139086Z","iopub.status.idle":"2024-10-25T23:08:55.726645Z","shell.execute_reply.started":"2024-10-25T23:08:55.139047Z","shell.execute_reply":"2024-10-25T23:08:55.725646Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"water 77\nsalad-leaf-salad-green 62\ntomato-raw 60\nbutter 41\nbread-white 39\nbread-wholemeal 36\ncoffee-with-caffeine 27\ncarrot-raw 27\ncucumber 26\nrice 24\nbread-whole-wheat 23\njam 21\negg 21\nbell-pepper-red-raw 19\napple 18\ncheese 17\nwhite-coffee-with-caffeine 17\nwine-white 16\nespresso-with-caffeine 14\npasta 13\nmixed-vegetables 13\nblueberries 13\nonion 13\nhummus 13\nbraided-white-loaf 13\nhard-cheese 12\nbread 12\nmixed-salad-chopped-without-sauce 12\nstrawberries 12\nsalmon-smoked 11\n","output_type":"stream"}]},{"cell_type":"code","source":"df_image_info.to_csv(\"/kaggle/working/val_data.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:10:01.142923Z","iopub.execute_input":"2024-10-25T23:10:01.143590Z","iopub.status.idle":"2024-10-25T23:10:01.159350Z","shell.execute_reply.started":"2024-10-25T23:10:01.143549Z","shell.execute_reply":"2024-10-25T23:10:01.158442Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"val_data = pd.read_csv(\"/kaggle/input/valdatacsv/val_data.csv\")\nval_data.columns, val_data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:14:30.937264Z","iopub.execute_input":"2024-10-25T23:14:30.938167Z","iopub.status.idle":"2024-10-25T23:14:30.960329Z","shell.execute_reply.started":"2024-10-25T23:14:30.938123Z","shell.execute_reply":"2024-10-25T23:14:30.959453Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(Index(['id', 'file_name', 'height', 'width', 'category'], dtype='object'),\n id           0\n file_name    0\n height       0\n width        0\n category     0\n dtype: int64)"},"metadata":{}}]},{"cell_type":"code","source":"len(val_data['category'].unique())","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:15:13.695762Z","iopub.execute_input":"2024-10-25T23:15:13.696507Z","iopub.status.idle":"2024-10-25T23:15:13.706074Z","shell.execute_reply.started":"2024-10-25T23:15:13.696467Z","shell.execute_reply":"2024-10-25T23:15:13.705148Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"406"},"metadata":{}}]},{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:15:43.228758Z","iopub.execute_input":"2024-10-25T23:15:43.229115Z","iopub.status.idle":"2024-10-25T23:15:45.094573Z","shell.execute_reply.started":"2024-10-25T23:15:43.229079Z","shell.execute_reply":"2024-10-25T23:15:45.093571Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport joblib\nimport pickle\nfrom fastprogress import master_bar, progress_bar\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import StepLR\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nprint(torchvision.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:15:53.027192Z","iopub.execute_input":"2024-10-25T23:15:53.027795Z","iopub.status.idle":"2024-10-25T23:15:55.090098Z","shell.execute_reply.started":"2024-10-25T23:15:53.027754Z","shell.execute_reply":"2024-10-25T23:15:55.089182Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"0.19.0\n","output_type":"stream"}]},{"cell_type":"code","source":"food_dataset = r'/kaggle/input/valdatacsv/val_data.csv'","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:16:18.652027Z","iopub.execute_input":"2024-10-25T23:16:18.652572Z","iopub.status.idle":"2024-10-25T23:16:18.656903Z","shell.execute_reply.started":"2024-10-25T23:16:18.652519Z","shell.execute_reply":"2024-10-25T23:16:18.655941Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# train_images_path = r'/kaggle/input/food-recognition-2022-dataset/public_training_set_release_2.0/images'\ntest_images_path = r'/kaggle/input/food-recognition-2022-dataset/public_test_release_2.0/images'\nval_images_path = r'/kaggle/input/food-recognition-2022-dataset/public_validation_set_2.0/images'","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:16:24.009328Z","iopub.execute_input":"2024-10-25T23:16:24.010237Z","iopub.status.idle":"2024-10-25T23:16:24.014521Z","shell.execute_reply.started":"2024-10-25T23:16:24.010185Z","shell.execute_reply":"2024-10-25T23:16:24.013291Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"IMG_WIDTH=224\nIMG_HEIGHT=224\nIMG_DIM = (IMG_WIDTH, IMG_HEIGHT)\nvalidation_ratio = 0.2\nbatch_size = 64\nwidth_shift_range=0.2,\n\nheight_shift_range=0.6,\nzoom_range=0.3, \nshear_range=0.4, \nrotation_range=10","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:16:58.858443Z","iopub.execute_input":"2024-10-25T23:16:58.859258Z","iopub.status.idle":"2024-10-25T23:16:58.864299Z","shell.execute_reply.started":"2024-10-25T23:16:58.859221Z","shell.execute_reply":"2024-10-25T23:16:58.863149Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class CustomImageDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        self.labels_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n        self.label_mapping = {label: idx for idx, label in enumerate(self.labels_frame['category'].unique())}\n        \n    def __len__(self):\n        return len(self.labels_frame)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, self.labels_frame.iloc[idx]['file_name'])\n        image = Image.open(img_name).convert(\"RGB\")\n        label_str = self.labels_frame.iloc[idx]['category']\n        label = self.label_mapping[label_str]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, torch.tensor(label)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:17:27.323927Z","iopub.execute_input":"2024-10-25T23:17:27.324598Z","iopub.status.idle":"2024-10-25T23:17:27.332294Z","shell.execute_reply.started":"2024-10-25T23:17:27.324556Z","shell.execute_reply":"2024-10-25T23:17:27.331368Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"csv_file = food_dataset\nroot_dir = '/kaggle/input/food-recognition-2022-dataset/public_validation_set_2.0/images/'\ndataset = CustomImageDataset(csv_file=csv_file, root_dir=root_dir)\n\nprint(f'Total images: {len(dataset)}')\nimg, label = dataset[0] \nprint(f'Image shape: {img.size}, Label: {label}')","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:18:13.520586Z","iopub.execute_input":"2024-10-25T23:18:13.520946Z","iopub.status.idle":"2024-10-25T23:18:13.580536Z","shell.execute_reply.started":"2024-10-25T23:18:13.520913Z","shell.execute_reply":"2024-10-25T23:18:13.579472Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Total images: 1830\nImage shape: (870, 870), Label: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((IMG_WIDTH, IMG_HEIGHT)),\n    transforms.RandomAffine(\n        degrees = rotation_range,\n        translate = (width_shift_range[0], height_shift_range[0]),\n        scale = (1 - zoom_range[0], 1 + zoom_range[0]),\n        shear = shear_range[0]\n    ),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:18:23.083883Z","iopub.execute_input":"2024-10-25T23:18:23.084219Z","iopub.status.idle":"2024-10-25T23:18:23.090586Z","shell.execute_reply.started":"2024-10-25T23:18:23.084187Z","shell.execute_reply":"2024-10-25T23:18:23.089550Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"dataset = CustomImageDataset(csv_file=csv_file, root_dir=root_dir, transform=transform)\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:18:26.138278Z","iopub.execute_input":"2024-10-25T23:18:26.139131Z","iopub.status.idle":"2024-10-25T23:18:26.160131Z","shell.execute_reply.started":"2024-10-25T23:18:26.139087Z","shell.execute_reply":"2024-10-25T23:18:26.159231Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"validation_data = 0.2\ntrain_size = int((1 - validation_data) * 1830)\ntest_size = 1830 - train_size\n\ntrain_size,test_size","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:18:53.230918Z","iopub.execute_input":"2024-10-25T23:18:53.231286Z","iopub.status.idle":"2024-10-25T23:18:53.237750Z","shell.execute_reply.started":"2024-10-25T23:18:53.231250Z","shell.execute_reply":"2024-10-25T23:18:53.236838Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(1464, 366)"},"metadata":{}}]},{"cell_type":"code","source":"class SimpleCNN(nn.Module):\n    def __init__(self, num_classes):\n        super(SimpleCNN, self).__init__()\n        self.model = torchvision.models.resnet50(pretrained=True) \n        num_ftrs = self.model.fc.in_features\n        self.model.fc = nn.Linear(num_ftrs, num_classes)  \n\n    def forward(self, x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:18:59.605194Z","iopub.execute_input":"2024-10-25T23:18:59.605551Z","iopub.status.idle":"2024-10-25T23:18:59.611262Z","shell.execute_reply.started":"2024-10-25T23:18:59.605517Z","shell.execute_reply":"2024-10-25T23:18:59.610340Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# class MyModel3(nn.Module):\n#     def __init__(self):\n#         super(MyModel3, self).__init__()\n#         no_Of_Filters = 32\n#         size_of_Filter = (2, 2)\n#         size_of_pool = (2, 2)\n#         no_Of_Nodes = 250\n\n#         self.conv1 = nn.Conv2d(3, no_Of_Filters, kernel_size=size_of_Filter, padding=1)\n#         self.pool = nn.MaxPool2d(kernel_size=size_of_pool)\n#         self.batch_norm1 = nn.BatchNorm2d(no_Of_Filters)\n\n#         self.conv2 = nn.Conv2d(no_Of_Filters, no_Of_Filters*2, kernel_size=size_of_Filter, padding=1)\n#         self.conv3 = nn.Conv2d(no_Of_Filters*2, no_Of_Filters*4, kernel_size=size_of_Filter, padding=1)\n#         self.batch_norm2 = nn.BatchNorm2d(no_Of_Filters)\n\n#         self.dropout = nn.Dropout(0.4)\n#         self.flatten = nn.Flatten()\n#         self.fc1 = nn.Linear(no_Of_Filters * 8 * 8, no_Of_Nodes)\n#         self.fc2 = nn.Linear(no_Of_Nodes, 498)\n\n#     def forward(self, x):\n#         x = F.relu(self.conv1(x))\n#         x = self.pool(x)\n#         x = self.batch_norm1(x)\n\n#         x = F.relu(self.conv2(x))\n#         x = self.pool(x)\n\n#         x = F.relu(self.conv3(x))\n#         x = self.pool(x)\n#         x = self.dropout(x)\n\n#         x = self.flatten(x)\n#         x = F.relu(self.fc1(x))\n#         x = self.dropout(x)\n#         x = self.fc2(x)\n\n#         return x\n\n# model2 = MyModel3()\n# model2.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:19:04.334260Z","iopub.execute_input":"2024-10-25T23:19:04.334634Z","iopub.status.idle":"2024-10-25T23:19:04.339744Z","shell.execute_reply.started":"2024-10-25T23:19:04.334593Z","shell.execute_reply":"2024-10-25T23:19:04.338804Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"num_classes = dataset.labels_frame['category'].nunique()  \n\nmodel = SimpleCNN(num_classes=num_classes)\n# model = model2\nmodel = model.to('cuda' if torch.cuda.is_available() else 'cpu') \n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:19:17.673557Z","iopub.execute_input":"2024-10-25T23:19:17.674381Z","iopub.status.idle":"2024-10-25T23:19:19.121900Z","shell.execute_reply.started":"2024-10-25T23:19:17.674340Z","shell.execute_reply":"2024-10-25T23:19:19.121103Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 173MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100):\n    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')  \n    for epoch in range(num_epochs):\n        model.train() \n        running_loss = 0.0\n\n        for images, labels in train_loader:\n            images = images.to('cuda' if torch.cuda.is_available() else 'cpu')  \n            labels = labels.to('cuda' if torch.cuda.is_available() else 'cpu')  \n\n            optimizer.zero_grad()  \n            outputs = model(images) \n            loss = criterion(outputs, labels)  \n            loss.backward() \n            optimizer.step()  \n\n            running_loss += loss.item() \n\n        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n\n        model.eval()\n        with torch.no_grad():\n            correct = 0\n            total = 0\n            for images, labels in val_loader:\n                images = images.to('cuda' if torch.cuda.is_available() else 'cpu')\n                labels = labels.to('cuda' if torch.cuda.is_available() else 'cpu')\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n            print(f'Validation Accuracy: {100 * correct / total:.2f}%')\n\ntrain_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:19:37.871194Z","iopub.execute_input":"2024-10-25T23:19:37.871660Z","iopub.status.idle":"2024-10-26T00:09:31.537029Z","shell.execute_reply.started":"2024-10-25T23:19:37.871620Z","shell.execute_reply":"2024-10-26T00:09:31.536180Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Epoch [1/100], Loss: 5.5837\nValidation Accuracy: 12.02%\nEpoch [2/100], Loss: 4.8018\nValidation Accuracy: 12.02%\nEpoch [3/100], Loss: 4.4533\nValidation Accuracy: 12.30%\nEpoch [4/100], Loss: 4.1153\nValidation Accuracy: 12.84%\nEpoch [5/100], Loss: 3.8112\nValidation Accuracy: 13.93%\nEpoch [6/100], Loss: 3.5032\nValidation Accuracy: 15.30%\nEpoch [7/100], Loss: 3.2047\nValidation Accuracy: 13.93%\nEpoch [8/100], Loss: 2.9770\nValidation Accuracy: 13.39%\nEpoch [9/100], Loss: 2.6975\nValidation Accuracy: 14.48%\nEpoch [10/100], Loss: 2.4934\nValidation Accuracy: 12.84%\nEpoch [11/100], Loss: 2.2557\nValidation Accuracy: 13.39%\nEpoch [12/100], Loss: 2.0777\nValidation Accuracy: 12.57%\nEpoch [13/100], Loss: 1.9277\nValidation Accuracy: 13.39%\nEpoch [14/100], Loss: 1.8003\nValidation Accuracy: 13.11%\nEpoch [15/100], Loss: 1.6865\nValidation Accuracy: 12.57%\nEpoch [16/100], Loss: 1.5578\nValidation Accuracy: 11.20%\nEpoch [17/100], Loss: 1.4934\nValidation Accuracy: 14.21%\nEpoch [18/100], Loss: 1.4350\nValidation Accuracy: 13.11%\nEpoch [19/100], Loss: 1.3988\nValidation Accuracy: 12.84%\nEpoch [20/100], Loss: 1.3609\nValidation Accuracy: 13.11%\nEpoch [21/100], Loss: 1.2885\nValidation Accuracy: 12.02%\nEpoch [22/100], Loss: 1.2581\nValidation Accuracy: 11.48%\nEpoch [23/100], Loss: 1.2514\nValidation Accuracy: 11.75%\nEpoch [24/100], Loss: 1.1960\nValidation Accuracy: 11.75%\nEpoch [25/100], Loss: 1.1854\nValidation Accuracy: 10.93%\nEpoch [26/100], Loss: 1.1616\nValidation Accuracy: 13.66%\nEpoch [27/100], Loss: 1.1818\nValidation Accuracy: 14.21%\nEpoch [28/100], Loss: 1.1180\nValidation Accuracy: 11.48%\nEpoch [29/100], Loss: 1.1378\nValidation Accuracy: 12.30%\nEpoch [30/100], Loss: 1.1053\nValidation Accuracy: 12.30%\nEpoch [31/100], Loss: 1.0907\nValidation Accuracy: 13.39%\nEpoch [32/100], Loss: 1.0727\nValidation Accuracy: 12.02%\nEpoch [33/100], Loss: 1.0758\nValidation Accuracy: 11.20%\nEpoch [34/100], Loss: 1.0752\nValidation Accuracy: 12.02%\nEpoch [35/100], Loss: 1.0311\nValidation Accuracy: 12.02%\nEpoch [36/100], Loss: 1.0425\nValidation Accuracy: 13.39%\nEpoch [37/100], Loss: 1.0460\nValidation Accuracy: 12.57%\nEpoch [38/100], Loss: 1.0420\nValidation Accuracy: 12.30%\nEpoch [39/100], Loss: 1.0264\nValidation Accuracy: 11.48%\nEpoch [40/100], Loss: 1.0159\nValidation Accuracy: 10.38%\nEpoch [41/100], Loss: 1.0158\nValidation Accuracy: 12.30%\nEpoch [42/100], Loss: 1.0286\nValidation Accuracy: 11.75%\nEpoch [43/100], Loss: 1.0120\nValidation Accuracy: 10.66%\nEpoch [44/100], Loss: 1.0045\nValidation Accuracy: 12.57%\nEpoch [45/100], Loss: 1.0070\nValidation Accuracy: 12.02%\nEpoch [46/100], Loss: 1.0066\nValidation Accuracy: 11.20%\nEpoch [47/100], Loss: 1.0071\nValidation Accuracy: 10.38%\nEpoch [48/100], Loss: 0.9765\nValidation Accuracy: 10.66%\nEpoch [49/100], Loss: 0.9925\nValidation Accuracy: 11.75%\nEpoch [50/100], Loss: 0.9609\nValidation Accuracy: 12.84%\nEpoch [51/100], Loss: 0.9789\nValidation Accuracy: 10.93%\nEpoch [52/100], Loss: 0.9784\nValidation Accuracy: 11.75%\nEpoch [53/100], Loss: 0.9598\nValidation Accuracy: 11.75%\nEpoch [54/100], Loss: 0.9593\nValidation Accuracy: 11.48%\nEpoch [55/100], Loss: 0.9428\nValidation Accuracy: 11.48%\nEpoch [56/100], Loss: 0.9584\nValidation Accuracy: 12.30%\nEpoch [57/100], Loss: 0.9406\nValidation Accuracy: 12.30%\nEpoch [58/100], Loss: 0.9509\nValidation Accuracy: 11.75%\nEpoch [59/100], Loss: 0.9403\nValidation Accuracy: 10.66%\nEpoch [60/100], Loss: 0.9571\nValidation Accuracy: 9.02%\nEpoch [61/100], Loss: 0.9684\nValidation Accuracy: 9.56%\nEpoch [62/100], Loss: 0.9521\nValidation Accuracy: 9.29%\nEpoch [63/100], Loss: 0.9343\nValidation Accuracy: 9.56%\nEpoch [64/100], Loss: 0.9386\nValidation Accuracy: 10.11%\nEpoch [65/100], Loss: 0.9456\nValidation Accuracy: 12.02%\nEpoch [66/100], Loss: 0.9264\nValidation Accuracy: 10.11%\nEpoch [67/100], Loss: 0.9247\nValidation Accuracy: 11.20%\nEpoch [68/100], Loss: 0.9190\nValidation Accuracy: 10.11%\nEpoch [69/100], Loss: 0.9157\nValidation Accuracy: 11.48%\nEpoch [70/100], Loss: 0.9318\nValidation Accuracy: 12.02%\nEpoch [71/100], Loss: 0.9458\nValidation Accuracy: 9.02%\nEpoch [72/100], Loss: 0.9134\nValidation Accuracy: 11.20%\nEpoch [73/100], Loss: 0.9193\nValidation Accuracy: 12.02%\nEpoch [74/100], Loss: 0.9039\nValidation Accuracy: 10.93%\nEpoch [75/100], Loss: 0.9006\nValidation Accuracy: 10.66%\nEpoch [76/100], Loss: 0.9200\nValidation Accuracy: 8.74%\nEpoch [77/100], Loss: 0.9377\nValidation Accuracy: 11.20%\nEpoch [78/100], Loss: 0.9060\nValidation Accuracy: 12.84%\nEpoch [79/100], Loss: 0.9160\nValidation Accuracy: 10.66%\nEpoch [80/100], Loss: 0.9068\nValidation Accuracy: 10.66%\nEpoch [81/100], Loss: 0.9199\nValidation Accuracy: 10.66%\nEpoch [82/100], Loss: 0.9164\nValidation Accuracy: 10.38%\nEpoch [83/100], Loss: 0.9103\nValidation Accuracy: 10.66%\nEpoch [84/100], Loss: 0.8983\nValidation Accuracy: 10.93%\nEpoch [85/100], Loss: 0.8910\nValidation Accuracy: 11.48%\nEpoch [86/100], Loss: 0.8977\nValidation Accuracy: 10.38%\nEpoch [87/100], Loss: 0.8969\nValidation Accuracy: 8.74%\nEpoch [88/100], Loss: 0.9129\nValidation Accuracy: 10.38%\nEpoch [89/100], Loss: 0.9300\nValidation Accuracy: 12.84%\nEpoch [90/100], Loss: 0.8981\nValidation Accuracy: 10.11%\nEpoch [91/100], Loss: 0.8841\nValidation Accuracy: 11.20%\nEpoch [92/100], Loss: 0.8818\nValidation Accuracy: 10.93%\nEpoch [93/100], Loss: 0.8666\nValidation Accuracy: 10.93%\nEpoch [94/100], Loss: 0.8761\nValidation Accuracy: 10.11%\nEpoch [95/100], Loss: 0.8850\nValidation Accuracy: 10.66%\nEpoch [96/100], Loss: 0.8769\nValidation Accuracy: 10.11%\nEpoch [97/100], Loss: 0.8856\nValidation Accuracy: 10.38%\nEpoch [98/100], Loss: 0.8740\nValidation Accuracy: 9.84%\nEpoch [99/100], Loss: 0.8828\nValidation Accuracy: 10.93%\nEpoch [100/100], Loss: 0.8877\nValidation Accuracy: 9.84%\n","output_type":"stream"}]},{"cell_type":"code","source":"num_epochs = 100\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        images = images.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:10:12.679038Z","iopub.execute_input":"2024-10-26T00:10:12.679476Z","iopub.status.idle":"2024-10-26T00:33:39.140363Z","shell.execute_reply.started":"2024-10-26T00:10:12.679431Z","shell.execute_reply":"2024-10-26T00:33:39.139375Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Epoch [1/100], Loss: 2.1292\nEpoch [2/100], Loss: 2.0453\nEpoch [3/100], Loss: 1.3786\nEpoch [4/100], Loss: 1.2422\nEpoch [5/100], Loss: 1.2219\nEpoch [6/100], Loss: 0.8664\nEpoch [7/100], Loss: 0.7911\nEpoch [8/100], Loss: 1.3925\nEpoch [9/100], Loss: 0.9634\nEpoch [10/100], Loss: 0.8732\nEpoch [11/100], Loss: 0.8929\nEpoch [12/100], Loss: 1.0228\nEpoch [13/100], Loss: 0.7585\nEpoch [14/100], Loss: 1.0953\nEpoch [15/100], Loss: 0.9322\nEpoch [16/100], Loss: 0.7810\nEpoch [17/100], Loss: 1.0033\nEpoch [18/100], Loss: 1.0146\nEpoch [19/100], Loss: 0.6216\nEpoch [20/100], Loss: 0.8615\nEpoch [21/100], Loss: 1.0239\nEpoch [22/100], Loss: 0.8810\nEpoch [23/100], Loss: 0.7376\nEpoch [24/100], Loss: 1.1829\nEpoch [25/100], Loss: 1.0403\nEpoch [26/100], Loss: 0.6517\nEpoch [27/100], Loss: 1.1428\nEpoch [28/100], Loss: 0.8678\nEpoch [29/100], Loss: 0.8200\nEpoch [30/100], Loss: 0.8641\nEpoch [31/100], Loss: 0.9103\nEpoch [32/100], Loss: 1.2333\nEpoch [33/100], Loss: 1.0544\nEpoch [34/100], Loss: 0.8082\nEpoch [35/100], Loss: 0.9481\nEpoch [36/100], Loss: 0.9676\nEpoch [37/100], Loss: 0.8114\nEpoch [38/100], Loss: 1.1196\nEpoch [39/100], Loss: 1.0587\nEpoch [40/100], Loss: 0.9789\nEpoch [41/100], Loss: 1.1178\nEpoch [42/100], Loss: 0.8021\nEpoch [43/100], Loss: 1.0138\nEpoch [44/100], Loss: 1.0148\nEpoch [45/100], Loss: 1.3500\nEpoch [46/100], Loss: 1.1483\nEpoch [47/100], Loss: 0.9193\nEpoch [48/100], Loss: 0.9355\nEpoch [49/100], Loss: 0.9913\nEpoch [50/100], Loss: 0.8795\nEpoch [51/100], Loss: 1.0139\nEpoch [52/100], Loss: 0.8446\nEpoch [53/100], Loss: 0.8037\nEpoch [54/100], Loss: 0.8197\nEpoch [55/100], Loss: 0.8801\nEpoch [56/100], Loss: 0.9613\nEpoch [57/100], Loss: 0.8663\nEpoch [58/100], Loss: 1.0268\nEpoch [59/100], Loss: 1.0112\nEpoch [60/100], Loss: 0.9018\nEpoch [61/100], Loss: 1.0747\nEpoch [62/100], Loss: 0.6221\nEpoch [63/100], Loss: 0.9640\nEpoch [64/100], Loss: 1.0077\nEpoch [65/100], Loss: 1.0144\nEpoch [66/100], Loss: 0.8653\nEpoch [67/100], Loss: 1.1064\nEpoch [68/100], Loss: 1.2105\nEpoch [69/100], Loss: 0.8906\nEpoch [70/100], Loss: 0.7593\nEpoch [71/100], Loss: 1.1276\nEpoch [72/100], Loss: 1.0790\nEpoch [73/100], Loss: 0.7869\nEpoch [74/100], Loss: 0.9228\nEpoch [75/100], Loss: 1.1454\nEpoch [76/100], Loss: 0.9682\nEpoch [77/100], Loss: 0.9799\nEpoch [78/100], Loss: 0.9390\nEpoch [79/100], Loss: 1.2307\nEpoch [80/100], Loss: 0.7137\nEpoch [81/100], Loss: 0.7745\nEpoch [82/100], Loss: 1.0653\nEpoch [83/100], Loss: 0.4935\nEpoch [84/100], Loss: 0.9058\nEpoch [85/100], Loss: 0.4731\nEpoch [86/100], Loss: 0.8999\nEpoch [87/100], Loss: 0.7772\nEpoch [88/100], Loss: 0.9146\nEpoch [89/100], Loss: 0.9149\nEpoch [90/100], Loss: 1.0064\nEpoch [91/100], Loss: 0.9529\nEpoch [92/100], Loss: 0.7743\nEpoch [93/100], Loss: 0.9499\nEpoch [94/100], Loss: 0.8932\nEpoch [95/100], Loss: 0.9740\nEpoch [96/100], Loss: 0.9815\nEpoch [97/100], Loss: 0.8209\nEpoch [98/100], Loss: 1.0436\nEpoch [99/100], Loss: 0.8228\nEpoch [100/100], Loss: 0.9458\n","output_type":"stream"}]},{"cell_type":"code","source":"with torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    print('Accuracy of the network on the {} train images: {} %'.format(1830, 100 * correct / total))","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:34:18.609929Z","iopub.execute_input":"2024-10-26T00:34:18.610948Z","iopub.status.idle":"2024-10-26T00:34:34.848559Z","shell.execute_reply.started":"2024-10-26T00:34:18.610904Z","shell.execute_reply":"2024-10-26T00:34:34.847595Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Accuracy of the network on the 1830 train images: 60.51912568306011 %\n","output_type":"stream"}]},{"cell_type":"code","source":"with open(\"/kaggle/working/data.pkl\", \"wb\") as file:\n    pickle.dump(model, file)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:40:16.439002Z","iopub.execute_input":"2024-10-26T00:40:16.439382Z","iopub.status.idle":"2024-10-26T00:40:16.663761Z","shell.execute_reply.started":"2024-10-26T00:40:16.439343Z","shell.execute_reply":"2024-10-26T00:40:16.662739Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"class ConvNeuralNet(nn.Module):\n    def __init__(self, num_classes):\n        super(ConvNeuralNet, self).__init__()\n        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3)\n        self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n        \n        self.conv_layer3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3)\n        self.conv_layer4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3)\n        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n        \n        self.conv_layer5 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size=3)\n        self.max_pool3 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n        \n        self.fc1 = nn.Linear(1600, 128)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(128, num_classes)\n      \n    def forward(self, x):\n        out = self.conv_layer1(x)\n        out = self.conv_layer2(out)\n        out = self.max_pool1(out)\n        \n        out = self.conv_layer3(out)\n        out = self.conv_layer4(out)\n        out = self.max_pool2(out)\n        \n        out = self.conv_layer5(out)\n        out = self.max_pool3(out)\n                \n        out = out.reshape(out.size(0), -1)\n        \n        out = self.fc1(out)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_model = ConvNeuralNet(num_classes)\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = torch.optim.SGD(custom_model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n\ntotal_step = len(train_loader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        images = images.to(device)\n        labels = labels.to(device)\n        \n        outputs = custom_model(images)\n        loss = criterion(outputs, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    print('Accuracy of the network on the {} train images: {} %'.format(1830, 100 * correct / total))","metadata":{},"execution_count":null,"outputs":[]}]}